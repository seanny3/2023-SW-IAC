{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import MNIST, ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import random\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils import imshow, show_plot\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Loss(대조 손실)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 샴네트워크 아키텍처 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샴네트워크 아키텍처 구성\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        \n",
    "        # Setting up the Sequential of CNN Layers\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 96, kernel_size=11,stride=1),\n",
    "            nn.BatchNorm2d(96),\n",
    "            #nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(96, 256, kernel_size=5,stride=1,padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "\n",
    "            nn.Conv2d(256,384 , kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(384,256 , kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nn.Dropout2d(p=0.3),\n",
    "\n",
    "        )\n",
    "        \n",
    "        # Defining the fully connected layers\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(30976, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            \n",
    "            nn.Linear(1024, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(128,2))\n",
    "  \n",
    "    def forward_once(self, x):\n",
    "        # Forward pass \n",
    "        output = self.cnn1(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # forward pass of input 1\n",
    "        output1 = self.forward_once(input1)\n",
    "        # forward pass of input 2\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetworkDataset(Dataset):\n",
    "    def __init__(self,imageFolderDataset,transform=None,should_invert=True):\n",
    "        self.imageFolderDataset = imageFolderDataset    \n",
    "        self.transform = transform\n",
    "        self.should_invert = should_invert\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "        # 약 50%의 확률로 같은 클래스 이미지를 선택할지, 다른 클래스 이미지를 선택할지를 결정\n",
    "        should_get_same_class = random.randint(0,1) \n",
    "        if should_get_same_class:\n",
    "            while True:\n",
    "                #keep looping till the same class image is found\n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1]==img1_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            while True:\n",
    "                #keep looping till a different class image is found\n",
    "                \n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1] !=img1_tuple[1]:\n",
    "                    break\n",
    "\n",
    "        img0 = Image.open(img0_tuple[0])\n",
    "        img1 = Image.open(img1_tuple[0])\n",
    "        img0 = img0.convert(\"L\")\n",
    "        img1 = img1.convert(\"L\")\n",
    "        \n",
    "        if self.should_invert:\n",
    "            img0 = PIL.ImageOps.invert(img0)\n",
    "            img1 = PIL.ImageOps.invert(img1)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "        \n",
    "        return (img0, img1 , torch.from_numpy(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 폴더 경로 설정\n",
    "data_folder = './database/train'\n",
    "\n",
    "# 이미지 변환 설정\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((105, 105)), transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "# 이미지 폴더 데이터셋 생성\n",
    "dataset = ImageFolder(data_folder, transform=transform)  # transform은 SiameseNetworkDataset에서 수행\n",
    "\n",
    "# SiameseNetworkDataset 생성\n",
    "train_dataloader = SiameseNetworkDataset(imageFolderDataset=dataset, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 30/30 [00:09<00:00,  3.08it/s]\n",
      "Epoch 2/20: 100%|██████████| 30/30 [00:09<00:00,  3.05it/s]\n",
      "Epoch 3/20: 100%|██████████| 30/30 [00:10<00:00,  2.82it/s]\n",
      "Epoch 4/20: 100%|██████████| 30/30 [00:10<00:00,  2.79it/s]\n",
      "Epoch 5/20: 100%|██████████| 30/30 [00:10<00:00,  2.73it/s]\n",
      "Epoch 6/20: 100%|██████████| 30/30 [00:10<00:00,  2.81it/s]\n",
      "Epoch 7/20: 100%|██████████| 30/30 [00:10<00:00,  2.87it/s]\n",
      "Epoch 8/20: 100%|██████████| 30/30 [00:10<00:00,  2.87it/s]\n",
      "Epoch 9/20: 100%|██████████| 30/30 [00:10<00:00,  2.75it/s]\n",
      "Epoch 10/20: 100%|██████████| 30/30 [00:10<00:00,  2.74it/s]\n",
      "Epoch 11/20: 100%|██████████| 30/30 [00:10<00:00,  2.97it/s]\n",
      "Epoch 12/20: 100%|██████████| 30/30 [00:10<00:00,  2.91it/s]\n",
      "Epoch 13/20: 100%|██████████| 30/30 [00:11<00:00,  2.68it/s]\n",
      "Epoch 14/20: 100%|██████████| 30/30 [00:10<00:00,  2.84it/s]\n",
      "Epoch 15/20: 100%|██████████| 30/30 [00:10<00:00,  2.86it/s]\n",
      "Epoch 16/20: 100%|██████████| 30/30 [00:10<00:00,  2.87it/s]\n",
      "Epoch 17/20: 100%|██████████| 30/30 [00:11<00:00,  2.69it/s]\n",
      "Epoch 18/20: 100%|██████████| 30/30 [00:10<00:00,  2.73it/s]\n",
      "Epoch 19/20: 100%|██████████| 30/30 [00:11<00:00,  2.63it/s]\n"
     ]
    }
   ],
   "source": [
    "net = SiameseNetwork()\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3, weight_decay=0.0005)\n",
    "epochs=20\n",
    "\n",
    "def train(train_dataloader, epoch):\n",
    "    loss=[] \n",
    "    counter=[]\n",
    "    iteration_number = 0\n",
    "\n",
    "    for i, data in tqdm(enumerate(train_dataloader,0), total=len(train_dataloader), desc=f\"Epoch {epoch}/{epochs}\"):\n",
    "        if i >= len(train_dataloader):\n",
    "            break\n",
    "        img0, img1 , label = data\n",
    "        img0 = torch.unsqueeze(img0, 1)  # Add channel dimension\n",
    "        img1 = torch.unsqueeze(img1, 1)  # Add channel dimension\n",
    "        optimizer.zero_grad()\n",
    "        output1,output2 = net(img0, img1)\n",
    "        loss_contrastive = criterion(output1,output2,label)\n",
    "        loss_contrastive.backward()\n",
    "        optimizer.step()\n",
    "        loss.append(loss_contrastive.item())\n",
    "\n",
    "    loss = np.array(loss)\n",
    "    return loss.mean()/len(train_dataloader)\n",
    "\n",
    "def eval(eval_dataloader):\n",
    "    loss=[] \n",
    "    counter=[]\n",
    "    iteration_number = 0\n",
    "    for i, data in enumerate(eval_dataloader,0):\n",
    "        img0, img1 , label = data\n",
    "        img0 = torch.unsqueeze(img0, 1)  # Add channel dimension\n",
    "        img1 = torch.unsqueeze(img1, 1)  # Add channel dimension\n",
    "        output1,output2 = net(img0, img1)\n",
    "        loss_contrastive = criterion(output1, output2, label)\n",
    "        loss.append(loss_contrastive.item())\n",
    "    loss = np.array(loss)\n",
    "    return loss.mean()/len(eval_dataloader)\n",
    "\n",
    "for epoch in range(1, 20):\n",
    "    best_eval_loss = 9999\n",
    "    train_loss = train(train_dataloader, epoch)\n",
    "\n",
    "    print(f\"Current loss {train_loss}\")\n",
    "\n",
    "    torch.save(net.state_dict(), \"best.pt\")\n",
    "    # eval_loss = eval(eval_dataloader)\n",
    "\n",
    "    # print(f\"Training loss{train_loss}\")\n",
    "    # print(\"-\"*20)\n",
    "    # print(f\"Eval loss{eval_loss}\")\n",
    "\n",
    "    # if eval_loss<best_eval_loss:\n",
    "    #     best_eval_loss = eval_loss\n",
    "    #     print(\"-\"*20)\n",
    "    #     print(f\"Best Eval loss{best_eval_loss}\")\n",
    "    #     torch.save(net.state_dict(), \"best.pt\")\n",
    "    #     print(\"Model Saved Successfully\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[164], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mbest.pt\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m      9\u001b[0m model\u001b[39m.\u001b[39meval()  \u001b[39m# Set the model to evaluation mode\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[39mprint\u001b[39m(test_dataloader[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m     12\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(test_dataloader,\u001b[39m0\u001b[39m), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(test_dataloader)):\n",
      "Cell \u001b[1;32mIn[116], line 21\u001b[0m, in \u001b[0;36mSiameseNetworkDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m         \u001b[39m#keep looping till a different class image is found\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m         img1_tuple \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39;49mchoice(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimageFolderDataset\u001b[39m.\u001b[39mimgs) \n\u001b[0;32m     22\u001b[0m         \u001b[39mif\u001b[39;00m img0_tuple[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39mimg1_tuple[\u001b[39m1\u001b[39m]:\n\u001b[0;32m     23\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((105, 105)), transforms.ToTensor()]\n",
    ")\n",
    "test_dataset = ImageFolder(\"./database/test/\", transform=transform)  # transform은 SiameseNetworkDataset에서 수행\n",
    "test_dataloader = SiameseNetworkDataset(imageFolderDataset=test_dataset, transform=transform)\n",
    "\n",
    "model = SiameseNetwork()\n",
    "model.load_state_dict(torch.load(\"best.pt\"))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "print(test_dataloader[0])\n",
    "count = 0\n",
    "for i, data in tqdm(enumerate(test_dataloader,0), total=len(test_dataloader)):\n",
    "    x0, x1, label = data\n",
    "    img0 = torch.unsqueeze(img0, 1)  # Add channel dimension\n",
    "    img1 = torch.unsqueeze(img1, 1)  # Add channel dimension\n",
    "    concat = torch.cat((x0, x1), 0)\n",
    "    output1, output2 = model(x0.to(device), x1.to(device))\n",
    "\n",
    "    eucledian_distance = F.pairwise_distance(output1, output2)\n",
    "\n",
    "    if label == torch.FloatTensor([[0]]):\n",
    "        label = \"Original Pair Of Signature\"\n",
    "    else:\n",
    "        label = \"Forged Pair Of Signature\"\n",
    "\n",
    "    imshow(torchvision.utils.make_grid(concat))\n",
    "    print(\"Predicted Eucledian Distance:-\", eucledian_distance.item())\n",
    "    print(\"Actual Label:-\", label)\n",
    "    count = count + 1\n",
    "    if count == 10:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8-py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
